# -*- coding: utf-8 -*-
"""Twitter Sentimental Analysis using ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-lWM3f8k6TZOtCFOFUU1-5zIB9Xmcu08
"""

# installing kaggle library - Purpose was to use API to extract large data from kaggle site
!pip install kaggle

"""**Upload Kaggle.json file**



"""

# configuring the path of kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""**Importing Twitter Sentiment Dataset**"""

# API to fetch the dataset from kaggle - API command that loads data directly
!kaggle datasets download -d kazanova/sentiment140

# Extracting the compressed dataset
from zipfile import ZipFile
dataset = '/content/sentiment140.zip'

with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

pip install flask numpy pandas nltk scikit-learn

pip install tweepy

"""**Importing the dependencies**"""

import numpy as np
import pandas as pd
import re                              #re=regular expression
from nltk.corpus import stopwords      #nltk = natural language tool kit
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer # to use textual data as numerical data
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Example dataset
data = pd.DataFrame({
    'text': ['I love this!', 'I hate this!', 'This is amazing', 'This is awful'],
    'label': [1, 0, 1, 0]
})

# Splitting data
X = data['text']
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Text vectorization
vectorizer = TfidfVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)

# Train a Logistic Regression model
model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

# Evaluate the model
X_test_vectorized = vectorizer.transform(X_test)
y_pred = model.predict(X_test_vectorized)
print("Accuracy:", accuracy_score(y_test, y_pred))

import nltk
nltk.download('stopwords')

#printing the stopwords in English - these words have meaning but they doesn't matter
print(stopwords.words('english'))

"""**Data Processing**"""

# loading the data from csv file to pandas dataframe
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1')

# Checking the number of rows and columns
twitter_data.shape

#Printing first 5 rows of the dataframe
twitter_data.head()

#naming the columns and reading dataset again
column_names = ['target','ids','date','flag','user','text']
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1',names=column_names)

twitter_data.head()

# Counting the number of missing values in the dataset
twitter_data.isnull().sum()

#checking the distribution of target column --> 0 means negative, 1 means Positive
twitter_data['target'].value_counts()

"""**Convert the target from 4 to 1**"""

twitter_data.replace({'target':{4:1}}, inplace=True)

#checking the distribution of target column
twitter_data['target'].value_counts()

"""**Stemming**
Stemming is the process of reducing a word to its root word
"""

port_stem = PorterStemmer()

def stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]',' ',content)
  stemmed_content = stemmed_content.lower()
  stemmed_content = stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  stemmed_content = ' '.join(stemmed_content)
  return stemmed_content

twitter_data['stemmed_content'] =  twitter_data['text'].apply(stemming) #54 minutes it will take to complete execution

twitter_data.head()

print(twitter_data['stemmed_content'])

print(twitter_data['target'])

# separating data and label
X = twitter_data['stemmed_content'].values
Y = twitter_data['target'].values

print(X)

print(Y)

"""**Splitting the data to traing and testing data**"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

print(X_train)

print(X_test)

print(Y_train)

print(Y_test)

#converting the textual data to numerical data
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

print(X_train)

print(X_test)

"""**Training the Machine Learning Model**

**Logistic** **Regression**
"""

model = LogisticRegression(max_iter=1000)

model.fit(X_train,Y_train)

"""**Model Evaluation**

**Accuracy Score**
"""

# accuracy score on the training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train, X_train_prediction)

print('Accuracy Score on the training data: ', training_data_accuracy)

# accuracy score on the testing data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test, X_test_prediction)

print('Accuracy Score on the testing data: ', test_data_accuracy)

"""**Model Accuracy = 77.8%**

**Saving the trained model**
"""

import pickle

filename = 'trained_model.sav'
pickle.dump(model, open(filename, 'wb'))
import pickle

# Save the model and vectorizer
pickle.dump(model, open('trained_model.sav', 'wb'))
pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))

"""**Using the saved model for future predictions**"""

#loading the saved model
loaded_model = pickle.load(open("/content/trained_model.sav", 'rb'))

X_new = X_test[200]
print(Y_test[200])

prediction = model.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Negative')
else:
  print('The news is Positive')

X_new = X_test[3]
print(Y_test[3])

prediction = model.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Negative')
else:
  print('The news is Positive')